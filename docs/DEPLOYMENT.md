# éƒ¨ç½²å’Œè¿ç»´æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† ChatGPT å›¢é˜Ÿé‚€è¯·ç®¡ç†ç³»ç»Ÿçš„éƒ¨ç½²ã€é…ç½®å’Œè¿ç»´æµç¨‹ã€‚

## ğŸ“‹ ç›®å½•

- [éƒ¨ç½²æ¦‚è¿°](#éƒ¨ç½²æ¦‚è¿°)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [Docker éƒ¨ç½²](#docker-éƒ¨ç½²)
- [æ‰‹åŠ¨éƒ¨ç½²](#æ‰‹åŠ¨éƒ¨ç½²)
- [äº‘æœåŠ¡éƒ¨ç½²](#äº‘æœåŠ¡éƒ¨ç½²)
- [é…ç½®ç®¡ç†](#é…ç½®ç®¡ç†)
- [æ•°æ®åº“ç®¡ç†](#æ•°æ®åº“ç®¡ç†)
- [ç›‘æ§å’Œæ—¥å¿—](#ç›‘æ§å’Œæ—¥å¿—)
- [å¤‡ä»½å’Œæ¢å¤](#å¤‡ä»½å’Œæ¢å¤)
- [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [è¿ç»´è„šæœ¬](#è¿ç»´è„šæœ¬)

## ğŸ¯ éƒ¨ç½²æ¦‚è¿°

### ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A[ç”¨æˆ·] --> B[Nginx åå‘ä»£ç†]
    B --> C[Next.js å‰ç«¯]
    B --> D[FastAPI åç«¯]
    D --> E[PostgreSQL æ•°æ®åº“]
    F[ç›‘æ§] --> D
    G[æ—¥å¿—æ”¶é›†] --> D
    H[å¤‡ä»½å­˜å‚¨] --> E

    subgraph "è´Ÿè½½å‡è¡¡"
        B
    end

    subgraph "åº”ç”¨å±‚"
        C
        D
    end

    subgraph "æ•°æ®å±‚"
        E
        H
    end

    subgraph "è¿ç»´å±‚"
        F
        G
    end
```

### éƒ¨ç½²æ–¹å¼

1. **Docker Compose** - æ¨èç”¨äºå¼€å‘å’Œä¸­å°å‹ç”Ÿäº§ç¯å¢ƒ
2. **æ‰‹åŠ¨éƒ¨ç½²** - é€‚ç”¨äºæœ‰ç‰¹æ®Šè¦æ±‚çš„è‡ªå®šä¹‰ç¯å¢ƒ
3. **äº‘æœåŠ¡éƒ¨ç½²** - é€‚ç”¨äºå¤§è§„æ¨¡å’Œé«˜å¯ç”¨éœ€æ±‚

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

#### æœ€ä½é…ç½®
- **CPU**: 2 æ ¸å¿ƒ
- **å†…å­˜**: 4GB RAM
- **å­˜å‚¨**: 20GB SSD
- **ç½‘ç»œ**: 100Mbps å¸¦å®½

#### æ¨èé…ç½®
- **CPU**: 4 æ ¸å¿ƒ
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 50GB SSD
- **ç½‘ç»œ**: 1Gbps å¸¦å®½

#### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+ / RHEL 8+
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **Git**: 2.30+

### ç«¯å£è¦æ±‚

| ç«¯å£ | ç”¨é€” | è¯´æ˜ |
|------|------|------|
| 80 | HTTP | Web è®¿é—® |
| 443 | HTTPS | å®‰å…¨ Web è®¿é—® |
| 8000 | åç«¯ API | FastAPI æœåŠ¡ |
| 3000 | å‰ç«¯ | Next.js å¼€å‘æœåŠ¡å™¨ |
| 5432 | æ•°æ®åº“ | PostgreSQL |
| 22 | SSH | è¿œç¨‹ç®¡ç† |

## ğŸ³ Docker éƒ¨ç½²

### 1. å¿«é€Ÿå¼€å§‹ï¼ˆç”Ÿäº§æ¨¡æ¿ï¼‰

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd "gpt invite"

# åˆ›å»ºç”Ÿäº§ç¯å¢ƒå˜é‡
cp cloud/.env.production.example cloud/.env
# ç¼–è¾‘ cloud/.envï¼Œè‡³å°‘è®¾ç½®ï¼šDOMAINã€ADMIN_INITIAL_PASSWORDã€SECRET_KEYã€ENCRYPTION_KEYã€DATABASE_URLã€REDIS_URL

# æ ¡éªŒé…ç½®ï¼ˆå¯é€‰ï¼‰
bash cloud/scripts/verify-prod.sh cloud/.env

# ä½¿ç”¨ç”Ÿäº§æ¨¡æ¿å¯åŠ¨ï¼ˆPostgreSQL + Redis + Backend + Frontend + Nginxï¼‰
docker compose -f cloud/docker-compose.prod.yml --env-file cloud/.env up -d
```

### 2. ç¯å¢ƒå˜é‡é…ç½®ï¼ˆä¸å½“å‰ä»£ç å¯¹é½ï¼‰

åˆ›å»º `cloud/.env` æ–‡ä»¶ï¼ˆç”Ÿäº§ï¼‰ï¼š

```env
# è¿è¡Œæ¨¡å¼
ENV=production
NODE_ENV=production
DOMAIN=your-domain.com

# æ•°æ®åº“ï¼ˆç”Ÿäº§å»ºè®® PostgreSQLï¼‰
DATABASE_URL=postgresql+psycopg2://postgres:your_secure_password@postgres:5432/invite_db
POSTGRES_DB=invite_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password

# ç®¡ç†å‘˜ä¸å¯†é’¥ï¼ˆå¿…å¡«ï¼‰
ADMIN_INITIAL_PASSWORD=replace-with-strong-password
SECRET_KEY=replace-with-strong-secret-key
# 32-byte base64ï¼ˆopenssl rand -base64 32ï¼‰
ENCRYPTION_KEY=

# Redis é™æµï¼ˆæ¨èç”Ÿäº§å¼€å¯ï¼‰
REDIS_URL=redis://redis:6379/0
RATE_LIMIT_ENABLED=true
RATE_LIMIT_NAMESPACE=gpt_invite:rate

# ç­–ç•¥
ADMIN_SESSION_TTL_SECONDS=604800
TOKEN_DEFAULT_TTL_DAYS=40
MAX_LOGIN_ATTEMPTS=5
LOGIN_LOCKOUT_DURATION=300

# å‰ç«¯æœåŠ¡ç«¯è®¿é—®åç«¯
BACKEND_URL=http://backend:8000

# SMTPï¼ˆå¯é€‰ï¼›å½“å‰åç«¯æœªä½¿ç”¨é‚®ä»¶å‘é€é€»è¾‘ï¼Œå¯ç•™ç©ºï¼‰
# SMTP_HOST=
# SMTP_PORT=
# SMTP_USER=
# SMTP_PASS=
```

### 3. Docker Compose é…ç½®

ç”Ÿäº§ä½¿ç”¨çš„ Compose å·²å†…ç½®åœ¨ä»“åº“ï¼š`cloud/docker-compose.prod.yml`ï¼ŒåŒ…å« PostgreSQLã€Redisã€åç«¯ã€å‰ç«¯ä¸ Nginxã€‚å»ºè®®ç›´æ¥ä½¿ç”¨è¯¥æ–‡ä»¶ï¼Œç»“åˆ `cloud/.env`ã€‚

### 4. Nginx é…ç½®

`nginx/nginx.conf`:

```nginx
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }

    upstream frontend {
        server frontend:3000;
    }

    # HTTP é‡å®šå‘åˆ° HTTPS
    server {
        listen 80;
        server_name your-domain.com www.your-domain.com;
        return 301 https://$server_name$request_uri;
    }

    # HTTPS é…ç½®
    server {
        listen 443 ssl http2;
        server_name your-domain.com www.your-domain.com;

        # SSL è¯ä¹¦é…ç½®
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;

        # å®‰å…¨å¤´
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";

        # å‰ç«¯è·¯ç”±
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # API è·¯ç”±
        location /api/ {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # WebSocket æ”¯æŒ
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # é™æ€æ–‡ä»¶ç¼“å­˜
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶
        client_max_body_size 10M;
    }
}
```

### 5. éƒ¨ç½²è„šæœ¬

- ç”Ÿäº§é…ç½®æ ¡éªŒè„šæœ¬ï¼š`cloud/scripts/verify-prod.sh`
- ï¼ˆå¯é€‰ï¼‰åœ¨ CI/CD ä¸­è°ƒç”¨ä¸Šè¿°è„šæœ¬ä¸ `docker compose -f cloud/docker-compose.prod.yml --env-file cloud/.env up -d` å®Œæˆä¸€é”®éƒ¨ç½²

> è¯´æ˜ï¼šæœ¬æ–‡ä»¶æ—©æœŸç‰ˆæœ¬ä¸­åŒ…å« PM2ã€Alembicã€ç›‘æ§ï¼ˆPrometheus/Grafanaï¼‰ã€ä¸Šä¼ å¤‡ä»½ç­‰ç¤ºä¾‹ï¼Œå½“å‰ä»“åº“æœªæä¾›ç›¸å…³è„šæœ¬ä¸ç›®å½•ï¼Œå·²ç§»é™¤æˆ–æ ‡æ³¨ä¸ºå¯é€‰æ‰©å±•ã€‚å»ºè®®ç›´æ¥ä½¿ç”¨ `cloud/docker-compose.prod.yml` è¿›è¡Œç”Ÿäº§éƒ¨ç½²ã€‚

## ğŸ”§ æ‰‹åŠ¨éƒ¨ç½²

### 1. æ•°æ®åº“å®‰è£…

#### PostgreSQL å®‰è£…

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib

# CentOS/RHEL
sudo yum install postgresql-server postgresql-contrib
sudo postgresql-setup initdb
sudo systemctl enable postgresql
sudo systemctl start postgresql
```

#### æ•°æ®åº“é…ç½®

```bash
# åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
sudo -u postgres psql
CREATE DATABASE invite_db;
CREATE USER invite_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE invite_db TO invite_user;
\q
```

### 2. åç«¯éƒ¨ç½²

```bash
# è¿›å…¥åç«¯ç›®å½•
cd cloud/backend

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
export DATABASE_URL="postgresql://invite_user:password@localhost:5432/invite_db"
export SECRET_KEY="your_secret_key"
export ENCRYPTION_KEY="your_encryption_key"

# è¿è¡Œæ•°æ®åº“è¿ç§»
alembic upgrade head

# å¯åŠ¨æœåŠ¡
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### 3. å‰ç«¯éƒ¨ç½²

```bash
# è¿›å…¥å‰ç«¯ç›®å½•
cd cloud/web

# å®‰è£…ä¾èµ–
pnpm install

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
pnpm build

# å¯åŠ¨æœåŠ¡
pnpm start
```

### 4. è¿›ç¨‹ç®¡ç†ï¼ˆå¯é€‰ï¼‰

å¦‚éœ€è„±ç¦»å®¹å™¨ä½¿ç”¨ PM2 ç­‰è¿›ç¨‹ç®¡ç†å™¨ï¼Œè‡ªè¡Œç¼–å†™é…ç½®å¹¶æ³¨æ„ä¸æœ¬é¡¹ç›®ç¯å¢ƒå˜é‡ä¿æŒä¸€è‡´ã€‚å®˜æ–¹æ¨èç”Ÿäº§éƒ¨ç½²æ–¹å¼ä»ä¸º `cloud/docker-compose.prod.yml`ã€‚

## â˜ï¸ äº‘æœåŠ¡éƒ¨ç½²

### 1. AWS éƒ¨ç½²

#### EC2 å®ä¾‹é…ç½®

```bash
# åˆ›å»º EC2 å®ä¾‹ (Ubuntu 20.04)
aws ec2 run-instances \
  --image-id ami-12345678 \
  --instance-type t3.medium \
  --key-name my-key-pair \
  --security-group-ids sg-12345678 \
  --subnet-id subnet-12345678 \
  --user-data file://user-data.sh \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=invite-app}]'
```

#### ç”¨æˆ·æ•°æ®è„šæœ¬ (`user-data.sh`)

```bash
#!/bin/bash
apt-get update
apt-get install -y docker.io docker-compose git

# æ‹‰å–é¡¹ç›®
git clone https://github.com/your-repo/invite-system.git
cd invite-system/cloud

# é…ç½®ç¯å¢ƒå˜é‡
cat > .env << EOF
DATABASE_URL=postgresql://postgres:password@postgres:5432/invite_db
POSTGRES_PASSWORD=your_secure_password
ADMIN_INITIAL_PASSWORD=your_admin_password
SECRET_KEY=your_secret_key
ENCRYPTION_KEY=your_encryption_key
EOF

# å¯åŠ¨æœåŠ¡
docker-compose up -d
```

#### RDS æ•°æ®åº“

```bash
# åˆ›å»º RDS å®ä¾‹
aws rds create-db-instance \
  --db-instance-identifier invite-db \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --master-username postgres \
  --master-user-password your_password \
  --allocated-storage 20 \
  --vpc-security-group-ids sg-12345678 \
  --db-subnet-group-name default
```

### 2. é˜¿é‡Œäº‘éƒ¨ç½²

#### ECS å®ä¾‹åˆ›å»º

```bash
# ä½¿ç”¨ ECS æ§åˆ¶å°æˆ– CLI åˆ›å»ºå®ä¾‹
# é…ç½®å®‰å…¨ç»„å¼€æ”¾ 80, 443, 22 ç«¯å£
# å®‰è£… Docker å’Œ Docker Compose
```

#### RDS æ•°æ®åº“

```bash
# åˆ›å»º RDS PostgreSQL å®ä¾‹
aliyun rds CreateDBInstance \
  --RegionId cn-hangzhou \
  --Engine PostgreSQL \
  --EngineVersion 13.0 \
  --DBInstanceClass pg.n2.small.1 \
  --DBInstanceStorage 20
```

### 3. è…¾è®¯äº‘éƒ¨ç½²

```bash
# åˆ›å»º CVM å®ä¾‹
# é…ç½®å®‰å…¨ç»„
# ä½¿ç”¨ Docker Compose éƒ¨ç½²
```

## âš™ï¸ é…ç½®ç®¡ç†

### 1. ç¯å¢ƒå˜é‡ç®¡ç†

#### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºé…ç½®ç›®å½•
mkdir -p /etc/invite-system
chmod 700 /etc/invite-system

# åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶
cat > /etc/invite-system/.env << EOF
# ç”Ÿäº§ç¯å¢ƒé…ç½®
NODE_ENV=production
DEBUG=false

# æ•°æ®åº“é…ç½® (ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†æœåŠ¡)
DATABASE_URL=postgresql://user:password@db-host:5432/dbname

# å¯†é’¥é…ç½® (ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡)
SECRET_KEY=\${INVITE_SECRET_KEY}
ENCRYPTION_KEY=\${INVITE_ENCRYPTION_KEY}
JWT_SECRET_KEY=\${INVITE_JWT_SECRET}

# ç®¡ç†å‘˜é…ç½®
ADMIN_INITIAL_PASSWORD=\${INVITE_ADMIN_PASSWORD}

# é‚®ä»¶é…ç½®
SMTP_HOST=\${INVITE_SMTP_HOST}
SMTP_PORT=\${INVITE_SMTP_PORT}
SMTP_USER=\${INVITE_SMTP_USER}
SMTP_PASS=\${INVITE_SMTP_PASS}
EOF

# è®¾ç½®æƒé™
chmod 600 /etc/invite-system/.env
chown app:app /etc/invite-system/.env
```

#### é…ç½®éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# scripts/validate-config.sh

echo "éªŒè¯ç¯å¢ƒé…ç½®..."

# æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
required_vars=(
    "DATABASE_URL"
    "SECRET_KEY"
    "ENCRYPTION_KEY"
    "ADMIN_INITIAL_PASSWORD"
)

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "é”™è¯¯: ç¯å¢ƒå˜é‡ $var æœªè®¾ç½®"
        exit 1
    fi
done

# æµ‹è¯•æ•°æ®åº“è¿æ¥
python -c "
import os
from sqlalchemy import create_engine
try:
    engine = create_engine(os.environ['DATABASE_URL'])
    with engine.connect() as conn:
        conn.execute('SELECT 1')
    print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')
except Exception as e:
    print(f'âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}')
    exit(1)
"

echo "âœ… é…ç½®éªŒè¯é€šè¿‡"
```

### 2. å¯†é’¥ç®¡ç†

#### HashiCorp Vault é›†æˆ

```bash
# å®‰è£… Vault
wget https://releases.hashicorp.com/vault/1.12.0/vault_1.12.0_linux_amd64.zip
unzip vault_1.12.0_linux_amd64.zip
sudo mv vault /usr/local/bin/

# é…ç½® Vault
vault login <token>

# å­˜å‚¨å¯†é’¥
vault kv put secret/invite-system \
    secret_key="your_secret_key" \
    encryption_key="your_encryption_key" \
    jwt_secret="your_jwt_secret" \
    admin_password="your_admin_password"

# è¯»å–å¯†é’¥
export SECRET_KEY=$(vault kv get -field=secret_key secret/invite-system)
export ENCRYPTION_KEY=$(vault kv get -field=encryption_key secret/invite-system)
```

#### AWS Secrets Manager

```bash
# åˆ›å»ºå¯†é’¥
aws secretsmanager create-secret \
  --name invite-system/credentials \
  --secret-string '{"SECRET_KEY":"your_secret_key","ENCRYPTION_KEY":"your_encryption_key"}'

# è¯»å–å¯†é’¥
SECRET_VALUE=$(aws secretsmanager get-secret-value \
  --secret-id invite-system/credentials \
  --query SecretString \
  --output text)

export SECRET_KEY=$(echo $SECRET_VALUE | jq -r '.SECRET_KEY')
export ENCRYPTION_KEY=$(echo $SECRET_VALUE | jq -r '.ENCRYPTION_KEY')
```

## ğŸ—„ï¸ æ•°æ®åº“ç®¡ç†

### 1. æ•°æ®åº“è¿ç§»

```bash
# åˆ›å»ºæ–°è¿ç§»
alembic revision --autogenerate -m "æè¿°å˜æ›´"

# åº”ç”¨è¿ç§»
alembic upgrade head

# å›æ»šè¿ç§»
alembic downgrade -1

# æŸ¥çœ‹è¿ç§»å†å²
alembic history

# æŸ¥çœ‹å½“å‰ç‰ˆæœ¬
alembic current
```

### 2. æ•°æ®åº“ä¼˜åŒ–

#### ç´¢å¼•ä¼˜åŒ–

```sql
-- åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_codes_batch_id ON codes(batch_id);
CREATE INDEX CONCURRENTLY idx_codes_status ON codes(is_used, expires_at);
CREATE INDEX CONCURRENTLY idx_users_email_team ON users(email, team_id);
CREATE INDEX CONCURRENTLY idx_mothers_status ON mothers(is_active, is_valid);

-- åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE mothers;
ANALYZE codes;
ANALYZE users;
ANALYZE teams;
```

#### æŸ¥è¯¢ä¼˜åŒ–

```sql
-- æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements
WHERE mean_time > 100
ORDER BY mean_time DESC
LIMIT 10;

-- æŸ¥çœ‹è¡¨å¤§å°
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### 3. æ•°æ®åº“å¤‡ä»½

#### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# scripts/backup-db.sh

BACKUP_DIR="/backup/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="invite_db"
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
pg_dump $DB_NAME | gzip > $BACKUP_DIR/backup_$DATE.sql.gz

# åˆ é™¤æ—§å¤‡ä»½
find $BACKUP_DIR -name "backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨ (å¯é€‰)
if command -v aws &> /dev/null; then
    aws s3 cp $BACKUP_DIR/backup_$DATE.sql.gz s3://your-backup-bucket/postgresql/
fi

echo "æ•°æ®åº“å¤‡ä»½å®Œæˆ: backup_$DATE.sql.gz"
```

#### å¤‡ä»½ Cron ä»»åŠ¡

```bash
# æ·»åŠ åˆ° crontab
crontab -e

# æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½
0 2 * * * /path/to/scripts/backup-db.sh

# æ¯å‘¨æ—¥å®Œæ•´å¤‡ä»½
0 2 * * 0 /path/to/scripts/full-backup.sh
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—ï¼ˆå¯é€‰ç¤ºä¾‹ï¼‰

### 1. åº”ç”¨ç›‘æ§

#### Prometheus é…ç½®

`monitoring/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'invite-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:80']
```

#### Grafana ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "é‚€è¯·ç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "API è¯·æ±‚ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "æ•°æ®åº“è¿æ¥æ•°",
        "type": "singlestat",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends",
            "legendFormat": "è¿æ¥æ•°"
          }
        ]
      }
    ]
  }
}
```

### 2. æ—¥å¿—ç®¡ç†

#### æ—¥å¿—é…ç½®

```python
# app/core/logging.py
import logging
import sys
from pathlib import Path

def setup_logging():
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("/app/logs")
    log_dir.mkdir(exist_ok=True)

    # é…ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_dir / "app.log")
    file_handler.setFormatter(formatter)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    # é…ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
```

#### æ—¥å¿—è½®è½¬

`/etc/logrotate.d/invite-system`:

```
/app/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 app app
    postrotate
        docker-compose exec backend kill -USR1 1
    endscript
}
```

### 3. å¥åº·æ£€æŸ¥

#### åç«¯å¥åº·æ£€æŸ¥

```python
# app/api/v1/health.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.core.redis import redis_client

router = APIRouter()

@router.get("/health")
async def health_check(db: Session = Depends(get_db)):
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "checks": {}
    }

    # æ£€æŸ¥æ•°æ®åº“
    try:
        db.execute("SELECT 1")
        health_status["checks"]["database"] = "healthy"
    except Exception as e:
        health_status["checks"]["database"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    # æ£€æŸ¥ Redis
    try:
        redis_client.ping()
        health_status["checks"]["redis"] = "healthy"
    except Exception as e:
        health_status["checks"]["redis"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    return health_status
```

## ğŸ’¾ å¤‡ä»½å’Œæ¢å¤ï¼ˆç¤ºä¾‹ï¼‰

### 1. è‡ªåŠ¨å¤‡ä»½

#### å®Œæ•´å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# scripts/full-backup.sh

BACKUP_BASE="/backup"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
BACKUP_DIR="$BACKUP_BASE/full_backup_$DATE"
mkdir -p $BACKUP_DIR

echo "å¼€å§‹å®Œæ•´å¤‡ä»½..."

# 1. æ•°æ®åº“å¤‡ä»½
echo "å¤‡ä»½æ•°æ®åº“..."
pg_dump invite_db | gzip > $BACKUP_DIR/database.sql.gz

# 2. æ–‡ä»¶å¤‡ä»½
echo "å¤‡ä»½ä¸Šä¼ æ–‡ä»¶..."
tar -czf $BACKUP_DIR/uploads.tar.gz /app/data/uploads

# 3. é…ç½®æ–‡ä»¶å¤‡ä»½
echo "å¤‡ä»½é…ç½®æ–‡ä»¶..."
cp /etc/invite-system/.env $BACKUP_DIR/env.backup

# 4. æ—¥å¿—å¤‡ä»½
echo "å¤‡ä»½æ—¥å¿—æ–‡ä»¶..."
tar -czf $BACKUP_DIR/logs.tar.gz /app/logs

# 5. åˆ›å»ºå¤‡ä»½æ¸…å•
echo "åˆ›å»ºå¤‡ä»½æ¸…å•..."
cat > $BACKUP_DIR/manifest.txt << EOF
å¤‡ä»½æ—¶é—´: $(date)
å¤‡ä»½ç±»å‹: å®Œæ•´å¤‡ä»½
ç³»ç»Ÿç‰ˆæœ¬: $(git rev-parse HEAD)
æ–‡ä»¶æ¸…å•:
- database.sql.gz: æ•°æ®åº“å¤‡ä»½
- uploads.tar.gz: ä¸Šä¼ æ–‡ä»¶å¤‡ä»½
- env.backup: ç¯å¢ƒé…ç½®å¤‡ä»½
- logs.tar.gz: æ—¥å¿—æ–‡ä»¶å¤‡ä»½
EOF

# 6. å‹ç¼©æ•´ä¸ªå¤‡ä»½
echo "å‹ç¼©å¤‡ä»½..."
cd $BACKUP_BASE
tar -czf full_backup_$DATE.tar.gz full_backup_$DATE/
rm -rf full_backup_$DATE/

# 7. ä¸Šä¼ åˆ°äº‘å­˜å‚¨
if command -v aws &> /dev/null; then
    echo "ä¸Šä¼ åˆ° AWS S3..."
    aws s3 cp $BACKUP_BASE/full_backup_$DATE.tar.gz s3://your-backup-bucket/full/
fi

# 8. æ¸…ç†æ—§å¤‡ä»½
echo "æ¸…ç†æ—§å¤‡ä»½..."
find $BACKUP_BASE -name "full_backup_*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "å®Œæ•´å¤‡ä»½å®Œæˆ: full_backup_$DATE.tar.gz"
```

### 2. æ¢å¤æµç¨‹

#### æ•°æ®åº“æ¢å¤

```bash
#!/bin/bash
# scripts/restore-db.sh

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup_file>"
    exit 1
fi

echo "å¼€å§‹æ¢å¤æ•°æ®åº“..."

# 1. åœæ­¢åº”ç”¨æœåŠ¡
docker-compose stop backend

# 2. è§£å‹å¤‡ä»½æ–‡ä»¶
gunzip -c $BACKUP_FILE > temp_restore.sql

# 3. æ¢å¤æ•°æ®åº“
psql -h localhost -U postgres -d invite_db < temp_restore.sql

# 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm temp_restore.sql

# 5. å¯åŠ¨åº”ç”¨æœåŠ¡
docker-compose start backend

echo "æ•°æ®åº“æ¢å¤å®Œæˆ"
```

#### å®Œæ•´ç³»ç»Ÿæ¢å¤

```bash
#!/bin/bash
# scripts/full-restore.sh

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup_tar_file>"
    exit 1
fi

echo "å¼€å§‹å®Œæ•´ç³»ç»Ÿæ¢å¤..."

# 1. åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose down

# 2. è§£å‹å¤‡ä»½æ–‡ä»¶
BACKUP_DIR="restore_$(date +%Y%m%d_%H%M%S)"
mkdir -p $BACKUP_DIR
tar -xzf $BACKUP_FILE -C $BACKUP_DIR

cd $BACKUP_DIR

# 3. æ¢å¤é…ç½®æ–‡ä»¶
echo "æ¢å¤é…ç½®æ–‡ä»¶..."
sudo cp env.backup /etc/invite-system/.env

# 4. æ¢å¤æ•°æ®åº“
echo "æ¢å¤æ•°æ®åº“..."
gunzip -c database.sql.gz | psql -h localhost -U postgres -d invite_db

# 5. æ¢å¤ä¸Šä¼ æ–‡ä»¶
echo "æ¢å¤ä¸Šä¼ æ–‡ä»¶..."
sudo tar -xzf uploads.tar.gz -C /

# 6. æ¢å¤æ—¥å¿—æ–‡ä»¶ (å¯é€‰)
echo "æ¢å¤æ—¥å¿—æ–‡ä»¶..."
sudo tar -xzf logs.tar.gz -C /

# 7. å¯åŠ¨æœåŠ¡
echo "å¯åŠ¨æœåŠ¡..."
cd /path/to/invite-system/cloud
docker-compose up -d

# 8. ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# 9. éªŒè¯æ¢å¤
if curl -f http://localhost:8000/health; then
    echo "âœ… ç³»ç»Ÿæ¢å¤æˆåŠŸ!"
else
    echo "âŒ ç³»ç»Ÿæ¢å¤å¤±è´¥"
    exit 1
fi

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
cd ..
rm -rf $BACKUP_DIR

echo "ç³»ç»Ÿæ¢å¤å®Œæˆ"
```

## ğŸ”’ å®‰å…¨é…ç½®

### 1. SSL/TLS é…ç½®

#### Let's Encrypt è¯ä¹¦

```bash
# å®‰è£… Certbot
sudo apt install certbot python3-certbot-nginx

# è·å–è¯ä¹¦
sudo certbot --nginx -d your-domain.com -d www.your-domain.com

# è‡ªåŠ¨ç»­æœŸ
sudo crontab -e
# æ·»åŠ ä»¥ä¸‹è¡Œ
0 12 * * * /usr/bin/certbot renew --quiet
```

#### SSL é…ç½®

```nginx
# nginx/ssl.conf
ssl_protocols TLSv1.2 TLSv1.3;
ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
ssl_prefer_server_ciphers off;
ssl_session_cache shared:SSL:10m;
ssl_session_timeout 10m;

# HSTS
add_header Strict-Transport-Security "max-age=31536000" always;

# OCSP Stapling
ssl_stapling on;
ssl_stapling_verify on;
resolver 8.8.8.8 8.8.4.4 valid=300s;
resolver_timeout 5s;
```

### 2. é˜²ç«å¢™é…ç½®

```bash
# UFW é…ç½®
sudo ufw enable
sudo ufw default deny incoming
sudo ufw default allow outgoing

# å…è®¸ SSH
sudo ufw allow ssh

# å…è®¸ HTTP/HTTPS
sudo ufw allow 80
sudo ufw allow 443

# æŸ¥çœ‹çŠ¶æ€
sudo ufw status
```

### 3. åº”ç”¨å®‰å…¨

#### å®‰å…¨å¤´é…ç½®

```python
# app/middleware/security.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware

def add_security_middleware(app: FastAPI):
    # å¼ºåˆ¶ HTTPS
    app.add_middleware(HTTPSRedirectMiddleware)

    # CORS é…ç½®
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["https://your-domain.com"],
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
```

#### è¾“å…¥éªŒè¯

```python
# app/utils/validators.py
import re
from typing import Optional

def validate_email(email: str) -> bool:
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None

def validate_access_token(token: str) -> bool:
    # ChatGPT access token æ ¼å¼éªŒè¯
    pattern = r'^sk-[a-zA-Z0-9]{48}$'
    return re.match(pattern, token) is not None

def sanitize_input(input_str: str) -> str:
    # ç§»é™¤æ½œåœ¨çš„æ¶æ„å­—ç¬¦
    dangerous_chars = ['<', '>', '"', "'", '&', 'script', 'javascript']
    sanitized = input_str
    for char in dangerous_chars:
        sanitized = sanitized.replace(char, '')
    return sanitized.strip()
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ•°æ®åº“ä¼˜åŒ–

```sql
-- è¿æ¥æ± é…ç½®
ALTER SYSTEM SET max_connections = 200;
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;

-- é‡å¯ PostgreSQL ä½¿é…ç½®ç”Ÿæ•ˆ
SELECT pg_reload_conf();
```

### 2. åº”ç”¨ä¼˜åŒ–

#### ç¼“å­˜é…ç½®

```python
# app/core/cache.py
import redis
from functools import wraps

redis_client = redis.Redis(host='redis', port=6379, db=0)

def cache_result(expire_time: int = 300):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return json.loads(cached_result)

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            redis_client.setex(
                cache_key,
                expire_time,
                json.dumps(result, default=str)
            )
            return result
        return wrapper
    return decorator
```

### 3. å‰ç«¯ä¼˜åŒ–

```typescript
// next.config.js
module.exports = {
  // å¯ç”¨å‹ç¼©
  compress: true,

  // é™æ€èµ„æºä¼˜åŒ–
  images: {
    domains: ['your-domain.com'],
    formats: ['image/webp', 'image/avif'],
  },

  // æ„å»ºä¼˜åŒ–
  webpack: (config, { isServer }) => {
    if (!isServer) {
      config.resolve.fallback.fs = false;
    }
    return config;
  },

  // å®éªŒæ€§åŠŸèƒ½
  experimental: {
    appDir: true,
    serverComponentsExternalPackages: ['@prisma/client'],
  },
}
```

## ğŸ“‹ è¿ç»´è„šæœ¬

### 1. æœåŠ¡ç®¡ç†

```bash
#!/bin/bash
# scripts/manage-services.sh

case "$1" in
    start)
        echo "å¯åŠ¨æœåŠ¡..."
        docker-compose up -d
        ;;
    stop)
        echo "åœæ­¢æœåŠ¡..."
        docker-compose down
        ;;
    restart)
        echo "é‡å¯æœåŠ¡..."
        docker-compose restart
        ;;
    status)
        echo "æœåŠ¡çŠ¶æ€:"
        docker-compose ps
        ;;
    logs)
        echo "æŸ¥çœ‹æ—¥å¿—:"
        docker-compose logs -f
        ;;
    update)
        echo "æ›´æ–°æœåŠ¡..."
        ./scripts/deploy.sh
        ;;
    *)
        echo "ç”¨æ³•: $0 {start|stop|restart|status|logs|update}"
        exit 1
        ;;
esac
```

### 2. å¥åº·æ£€æŸ¥

```bash
#!/bin/bash
# scripts/health-check.sh

echo "æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥..."

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
services=("postgres" "redis" "backend" "frontend" "nginx")
for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "âœ… $service: è¿è¡Œä¸­"
    else
        echo "âŒ $service: æœªè¿è¡Œ"
    fi
done

# æ£€æŸ¥ API å¥åº·çŠ¶æ€
if curl -f http://localhost:8000/health &> /dev/null; then
    echo "âœ… API: å¥åº·"
else
    echo "âŒ API: ä¸å¥åº·"
fi

# æ£€æŸ¥å‰ç«¯
if curl -f http://localhost:3000 &> /dev/null; then
    echo "âœ… å‰ç«¯: å¯è®¿é—®"
else
    echo "âŒ å‰ç«¯: ä¸å¯è®¿é—®"
fi

# æ£€æŸ¥ç£ç›˜ç©ºé—´
disk_usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $disk_usage -gt 80 ]; then
    echo "âš ï¸  ç£ç›˜ä½¿ç”¨ç‡: ${disk_usage}% (è­¦å‘Š)"
else
    echo "âœ… ç£ç›˜ä½¿ç”¨ç‡: ${disk_usage}%"
fi

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
memory_usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
if [ $memory_usage -gt 80 ]; then
    echo "âš ï¸  å†…å­˜ä½¿ç”¨ç‡: ${memory_usage}% (è­¦å‘Š)"
else
    echo "âœ… å†…å­˜ä½¿ç”¨ç‡: ${memory_usage}%"
fi

echo "å¥åº·æ£€æŸ¥å®Œæˆ"
```

### 3. æ—¥å¿—åˆ†æ

```bash
#!/bin/bash
# scripts/analyze-logs.sh

LOG_DIR="/app/logs"
TODAY=$(date +%Y-%m-%d)

echo "åˆ†æä»Šæ—¥æ—¥å¿— ($TODAY)..."

# é”™è¯¯ç»Ÿè®¡
echo "é”™è¯¯ç»Ÿè®¡:"
grep -i "error" $LOG_DIR/app.log | grep $TODAY | wc -l

# è­¦å‘Šç»Ÿè®¡
echo "è­¦å‘Šç»Ÿè®¡:"
grep -i "warning" $LOG_DIR/app.log | grep $TODAY | wc -l

# API è¯·æ±‚ç»Ÿè®¡
echo "API è¯·æ±‚ç»Ÿè®¡:"
grep "POST\|GET\|PUT\|DELETE" $LOG_DIR/app.log | grep $TODAY | wc -l

# å“åº”æ—¶é—´ç»Ÿè®¡
echo "å¹³å‡å“åº”æ—¶é—´:"
grep "response_time" $LOG_DIR/app.log | grep $TODAY | awk '{print $NF}' | awk '{sum+=$1; count++} END {if(count>0) print sum/count " ms"}'

# é”™è¯¯è¯¦æƒ…
echo "ä»Šæ—¥é”™è¯¯è¯¦æƒ…:"
grep -i "error" $LOG_DIR/app.log | grep $TODAY | tail -10
```

### 4. æ€§èƒ½ç›‘æ§

```bash
#!/bin/bash
# scripts/performance-monitor.sh

echo "ç³»ç»Ÿæ€§èƒ½ç›‘æ§ - $(date)"

# CPU ä½¿ç”¨ç‡
cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
echo "CPU ä½¿ç”¨ç‡: ${cpu_usage}%"

# å†…å­˜ä½¿ç”¨
memory_info=$(free -h | awk 'NR==2{printf "å†…å­˜ä½¿ç”¨: %s/%s (%.1f%%)", $3,$2,$3*100/$2}')
echo $memory_info

# ç£ç›˜ I/O
disk_io=$(iostat -x 1 1 | awk 'NR==4 {print "ç£ç›˜ I/O: " $10 "% è¯»å–, " $14 "% å†™å…¥"}')
echo $disk_io

# ç½‘ç»œæµé‡
network=$(cat /proc/net/dev | grep eth0 | awk '{print "ç½‘ç»œæµé‡: æ¥æ”¶ " $2 " å­—èŠ‚, å‘é€ " $10 " å­—èŠ‚"}')
echo $network

# æ•°æ®åº“è¿æ¥æ•°
db_connections=$(psql -U postgres -d invite_db -t -c "SELECT count(*) FROM pg_stat_activity;")
echo "æ•°æ®åº“è¿æ¥æ•°: $db_connections"

# Redis å†…å­˜ä½¿ç”¨
redis_memory=$(redis-cli info memory | grep used_memory_human | cut -d: -f2 | tr -d '\r')
echo "Redis å†…å­˜ä½¿ç”¨: $redis_memory"
```

---

## ğŸ“ æ•…éšœå¤„ç†

### å¸¸è§é—®é¢˜è§£å†³

1. **æœåŠ¡æ— æ³•å¯åŠ¨**
   - æ£€æŸ¥ç«¯å£å ç”¨: `netstat -tlnp | grep :8000`
   - æŸ¥çœ‹æ—¥å¿—: `docker-compose logs backend`
   - æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®

2. **æ•°æ®åº“è¿æ¥å¤±è´¥**
   - æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
   - éªŒè¯è¿æ¥å­—ç¬¦ä¸²
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

3. **å‰ç«¯æ— æ³•è®¿é—®åç«¯**
   - æ£€æŸ¥ Nginx é…ç½®
   - éªŒè¯ CORS è®¾ç½®
   - æ£€æŸ¥ç½‘ç»œè¿æ¥

å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·æŸ¥çœ‹ [æ•…éšœæ’é™¤æŒ‡å—](./TROUBLESHOOTING.md) æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚
